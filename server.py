"""
FastAPI æœåŠ¡å…¥å£
å°† LangGraph Search Agent åŒ…è£…ä¸º REST API æœåŠ¡
"""
import json
import asyncio
from contextlib import asynccontextmanager
from typing import AsyncGenerator

from fastapi import FastAPI, HTTPException, Request
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field

from langchain_core.messages import SystemMessage, HumanMessage


# ============================================================
# Pydantic æ•°æ®æ¨¡å‹
# ============================================================

class ResearchRequest(BaseModel):
    """ç ”ç©¶è¯·æ±‚æ¨¡å‹"""
    topic: str = Field(..., description="ç ”ç©¶ä¸»é¢˜", min_length=1)
    user_id: str = Field(..., description="ç”¨æˆ·ID", min_length=1)
    thread_id: str = Field(..., description="ä¼šè¯IDï¼Œç”¨äºéš”ç¦»ä¸åŒå¯¹è¯", min_length=1)


class ResearchResponse(BaseModel):
    """ç ”ç©¶å“åº”æ¨¡å‹ï¼ˆåŒæ­¥ç«¯ç‚¹ä½¿ç”¨ï¼‰"""
    current_draft: str = Field(..., description="æœ€ç»ˆç”Ÿæˆçš„è‰ç¨¿")
    status: str = Field(default="completed", description="ä»»åŠ¡çŠ¶æ€")
    score: int = Field(default=0, description="Critic è¯„åˆ†")


class ErrorResponse(BaseModel):
    """é”™è¯¯å“åº”æ¨¡å‹"""
    error: str = Field(..., description="é”™è¯¯ç±»å‹")
    message: str = Field(..., description="é”™è¯¯è¯¦æƒ…")
    thread_id: str | None = Field(default=None, description="ç›¸å…³çš„ä¼šè¯ID")


# ============================================================
# è‡ªå®šä¹‰å¼‚å¸¸
# ============================================================

class LLMError(Exception):
    """LLM è°ƒç”¨ç›¸å…³é”™è¯¯"""
    def __init__(self, message: str, thread_id: str | None = None):
        self.message = message
        self.thread_id = thread_id
        super().__init__(self.message)


class ToolError(Exception):
    """å·¥å…·æ‰§è¡Œç›¸å…³é”™è¯¯"""
    def __init__(self, message: str, tool_name: str | None = None):
        self.message = message
        self.tool_name = tool_name
        super().__init__(self.message)


# ============================================================
# FastAPI åº”ç”¨
# ============================================================

@asynccontextmanager
async def lifespan(app: FastAPI):
    """
    åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†
    åœ¨å¯åŠ¨æ—¶åˆå§‹åŒ– LangGraph å®ä¾‹ï¼Œå…³é—­æ—¶è¿›è¡Œæ¸…ç†
    """
    # å¯åŠ¨æ—¶ï¼šå¯¼å…¥å¹¶ç¼“å­˜ LangGraph åº”ç”¨
    from graph import app as langgraph_app
    
    # å°† LangGraph å®ä¾‹å­˜å‚¨åœ¨ app.state ä¸­
    app.state.langgraph_app = langgraph_app
    
    print("âœ… LangGraph Search Agent å·²åˆå§‹åŒ–")
    
    yield
    
    # å…³é—­æ—¶ï¼šæ¸…ç†èµ„æºï¼ˆå¦‚æœ‰å¿…è¦ï¼‰
    print("ğŸ‘‹ æ­£åœ¨å…³é—­æœåŠ¡...")


app = FastAPI(
    title="Search Agent API",
    description="åŸºäº LangGraph çš„æ™ºèƒ½ç ”ç©¶åŠ©æ‰‹ API",
    version="1.0.0",
    lifespan=lifespan
)


# ============================================================
# å¼‚å¸¸å¤„ç†å™¨
# ============================================================

@app.exception_handler(LLMError)
async def llm_error_handler(request: Request, exc: LLMError):
    """å¤„ç† LLM ç›¸å…³é”™è¯¯"""
    return HTTPException(
        status_code=503,
        detail=ErrorResponse(
            error="llm_error",
            message=exc.message,
            thread_id=exc.thread_id
        ).model_dump()
    )


@app.exception_handler(ToolError)
async def tool_error_handler(request: Request, exc: ToolError):
    """å¤„ç†å·¥å…·æ‰§è¡Œé”™è¯¯"""
    return HTTPException(
        status_code=500,
        detail=ErrorResponse(
            error="tool_error",
            message=f"å·¥å…· {exc.tool_name} æ‰§è¡Œå¤±è´¥: {exc.message}"
        ).model_dump()
    )


# ============================================================
# SSE æµå¼æ ¼å¼åŒ–
# ============================================================

def format_sse_event(node: str, content: str, event_type: str = "message") -> str:
    """
    å°†äº‹ä»¶æ ¼å¼åŒ–ä¸º SSE æ ¼å¼
    
    Args:
        node: èŠ‚ç‚¹åç§° (writer, critic, tools_writer, etc.)
        content: æ¶ˆæ¯å†…å®¹
        event_type: äº‹ä»¶ç±»å‹
    
    Returns:
        SSE æ ¼å¼å­—ç¬¦ä¸²: data: {"node": "...", "content": "..."}\n\n
    """
    data = json.dumps({
        "node": node,
        "content": content,
        "type": event_type
    }, ensure_ascii=False)
    return f"data: {data}\n\n"


# ============================================================
# ç³»ç»Ÿæç¤ºè¯
# ============================================================

SYSTEM_PROMPT = """ä½ æ˜¯ä¸€åæ™ºèƒ½ç ”ç©¶åŠ©æ‰‹ã€‚
ä½ çš„ç›®æ ‡æ˜¯åˆ©ç”¨ç½‘ç»œæœç´¢å’Œç½‘é¡µè®¿é—®å·¥å…·æ¥ç ”ç©¶ç”¨æˆ·çš„ä¸»é¢˜ã€‚
1. ä½¿ç”¨ `search_web` æŸ¥æ‰¾ç›¸å…³é¡µé¢ã€‚
2. ä½¿ç”¨ `visit_page` é˜…è¯»æœ‰ä»·å€¼çš„ URL çš„è¯¦ç»†å†…å®¹ã€‚
3. å°†ä¿¡æ¯æ•´åˆæˆä¸€ä»½å…¨é¢çš„ Markdown æŠ¥å‘Šã€‚
4. å®Œæˆåï¼Œè¾“å‡ºæœ€ç»ˆçš„ Markdown æ ¼å¼æŠ¥å‘Šã€‚
5. è¯·åŠ¡å¿…ç”¨ä¸­æ–‡å›ç­”ã€‚
"""


# ============================================================
# API ç«¯ç‚¹
# ============================================================

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    return {"status": "healthy"}


@app.post("/api/research/stream")
async def research_stream(request: ResearchRequest):
    """
    æµå¼ç ”ç©¶ç«¯ç‚¹ (SSE)
    
    å®æ—¶æµå¼è¾“å‡º LangGraph æ‰§è¡Œè¿‡ç¨‹ä¸­çš„æ¯ä¸ªèŠ‚ç‚¹äº‹ä»¶ã€‚
    
    ä½¿ç”¨ç¤ºä¾‹:
    ```bash
    curl -X POST http://localhost:8000/api/research/stream \
      -H "Content-Type: application/json" \
      -H "Accept: text/event-stream" \
      -d '{"topic": "é‡å­è®¡ç®—", "user_id": "user1", "thread_id": "thread1"}'
    ```
    """
    
    async def event_generator() -> AsyncGenerator[str, None]:
        """ç”Ÿæˆ SSE äº‹ä»¶æµ"""
        langgraph_app = app.state.langgraph_app
        
        # æ„å»ºåˆå§‹è¾“å…¥
        initial_input = {
            "writer_messages": [
                SystemMessage(content=SYSTEM_PROMPT),
                HumanMessage(content=request.topic)
            ]
        }
        
        # é…ç½® thread_id ä»¥éš”ç¦»ä¼šè¯çŠ¶æ€
        config = {
            "configurable": {
                "thread_id": request.thread_id
            },
            "recursion_limit": 80
        }
        
        try:
            # å‘é€å¼€å§‹äº‹ä»¶
            yield format_sse_event("system", f"å¼€å§‹ç ”ç©¶: {request.topic}", "start")
            
            # ä½¿ç”¨ stream_mode="updates" æµå¼å¤„ç†
            for event in langgraph_app.stream(initial_input, stream_mode="updates", config=config):
                for node_name, node_val in event.items():
                    # å¤„ç† Writer æ¶ˆæ¯
                    if "writer_messages" in node_val and node_val["writer_messages"]:
                        message = node_val["writer_messages"][-1]
                        content = ""
                        
                        # æå–æ¶ˆæ¯å†…å®¹
                        if hasattr(message, 'content') and message.content:
                            content = str(message.content)
                        
                        # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
                        if hasattr(message, 'tool_calls') and message.tool_calls:
                            tool_info = [tc.get('name', 'unknown') for tc in message.tool_calls]
                            content = f"è°ƒç”¨å·¥å…·: {', '.join(tool_info)}"
                        
                        if content:
                            yield format_sse_event(node_name, content)
                    
                    # å¤„ç† Critic æ¶ˆæ¯
                    elif "critic_messages" in node_val and node_val["critic_messages"]:
                        message = node_val["critic_messages"][-1]
                        content = ""
                        
                        if hasattr(message, 'content') and message.content:
                            content = str(message.content)
                        
                        if hasattr(message, 'tool_calls') and message.tool_calls:
                            tool_info = [tc.get('name', 'unknown') for tc in message.tool_calls]
                            content = f"éªŒè¯å·¥å…·: {', '.join(tool_info)}"
                        
                        if content:
                            yield format_sse_event(node_name, content)
                    
                    # å¤„ç†è‰ç¨¿æ›´æ–°
                    if "current_draft" in node_val and node_val["current_draft"]:
                        yield format_sse_event("draft_update", node_val["current_draft"], "draft")
                    
                    # å¤„ç†è¯„åˆ†æ›´æ–°
                    if "score" in node_val:
                        yield format_sse_event("score", str(node_val["score"]), "score")
                
                # è®©å‡ºæ§åˆ¶æƒï¼Œé¿å…é˜»å¡
                await asyncio.sleep(0)
            
            # å‘é€å®Œæˆäº‹ä»¶
            yield format_sse_event("system", "ç ”ç©¶å®Œæˆ", "complete")
            
        except Exception as e:
            # å‘é€é”™è¯¯äº‹ä»¶
            yield format_sse_event("error", str(e), "error")
            raise
    
    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"
        }
    )


@app.post("/api/research/sync", response_model=ResearchResponse)
async def research_sync(request: ResearchRequest):
    """
    åŒæ­¥ç ”ç©¶ç«¯ç‚¹
    
    é˜»å¡å¼æ‰§è¡Œç ”ç©¶ä»»åŠ¡ï¼Œè¿”å›æœ€ç»ˆè‰ç¨¿ã€‚
    é€‚ç”¨äºä¸éœ€è¦å®æ—¶åé¦ˆçš„åœºæ™¯ã€‚
    
    æ³¨æ„: æ­¤ç«¯ç‚¹å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´æ‰èƒ½è¿”å›ï¼ˆå–å†³äºç ”ç©¶å¤æ‚åº¦ï¼‰ã€‚
    
    ä½¿ç”¨ç¤ºä¾‹:
    ```bash
    curl -X POST http://localhost:8000/api/research/sync \
      -H "Content-Type: application/json" \
      -d '{"topic": "äººå·¥æ™ºèƒ½å‘å±•è¶‹åŠ¿", "user_id": "user1", "thread_id": "thread2"}'
    ```
    """
    langgraph_app = app.state.langgraph_app
    
    # æ„å»ºåˆå§‹è¾“å…¥
    initial_input = {
        "writer_messages": [
            SystemMessage(content=SYSTEM_PROMPT),
            HumanMessage(content=request.topic)
        ]
    }
    
    # é…ç½® thread_id
    config = {
        "configurable": {
            "thread_id": request.thread_id
        },
        "recursion_limit": 80
    }
    
    try:
        # ä½¿ç”¨ invoke åŒæ­¥æ‰§è¡Œ
        final_state = langgraph_app.invoke(initial_input, config=config)
        
        return ResearchResponse(
            current_draft=final_state.get("current_draft", ""),
            status="completed",
            score=final_state.get("score", 0)
        )
    
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=ErrorResponse(
                error="execution_error",
                message=str(e),
                thread_id=request.thread_id
            ).model_dump()
        )


# ============================================================
# ä¸»å…¥å£
# ============================================================

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "server:app",
        host="0.0.0.0",
        port=8000,
        reload=True
    )
